# Project: Text field classifier (5 classes)# This file bundles multiple module files into one document for convenience.# Each code line is preceded by a comment explaining what it does and why.
# --------------------# file: requirements.txt# --------------------# The following packages are required to run this project.# torch: core deep learning framework (PyTorch) used for model definition and training.# transformers: Hugging Face Transformers for tokenizers and pre-trained models.# datasets: utilities and dataset handling (optional but useful).# scikit-learn: evaluation metrics like classification_report and confusion_matrix.# pandas: data manipulation and CSV I/O.# tqdm: progress bars for longer-running loops.# regex: improved regex support for text processing.# torch>=1.13# transformers>=4.30# datasets>=2.0# scikit-learn# pandas# tqdm# regex
# --------------------# file: README.md# --------------------# A concise README that explains usage, pipeline steps, and practical notes about labeling and# model choices. Keep the README with the repo so other engineers understand how to run the system.
# --------------------# file: data_prep.py# --------------------# Import regular expressions module for string pattern matching and substitution.import re# Import argparse to parse command-line arguments when the script is executed directly.import argparse# Import pandas for reading and writing CSVs and for dataframe manipulation.import pandas as pd# Import tqdm for progress bars to monitor long loops (improves UX while labeling many rows).from tqdm import tqdm
# Define the set of labels we expect. 'unknown' is a catch-all for things heuristics can't decide.LABELS = [    'serial_number',    'make',    'customer_id',    'dealer_code',    'dealer_customer_number',    'unknown']
# Heuristic function: determine if a token looks like a serial number.# Serial numbers are typically alphanumeric, reasonably long, and have both letters and digits.def is_serial(s):    # Remove non-alphanumeric characters to focus on the core token.    s_clean = re.sub(r"[^A-Za-z0-9]","", s)    # Return True when length >= 8 and contains both letters and digits.    return len(s_clean) >= 8 and re.search(r"[A-Za-z]", s_clean) and re.search(r"[0-9]", s_clean)
# Heuristic function: determine if token looks like a customer ID.# Customer IDs are often numeric or prefix+digits (e.g., CUST12345) and moderate length.def is_customer_id(s):    # Strip non-alphanumerics to standardize evaluation.    s_clean = re.sub(r"[^A-Za-z0-9]","", s)    # Check common patterns: purely digits of reasonable size, or a CUST prefix.    return (s_clean.isdigit() and 5 <= len(s_clean) <= 12) or re.match(r"^CUST[0-9]+$", s_clean, re.I)
# Heuristic function: detect dealer codes which tend to be short uppercase alpha/numeric codes.def is_dealer_code(s):    # Remove non-alphanumeric characters.    s_clean = re.sub(r"[^A-Za-z0-9]","", s)    # Dealer codes are usually short (2-6 chars) and uppercase alphanumeric.    return 2 <= len(s_clean) <= 6 and re.match(r"^[A-Z0-9]{2,6}$", s_clean)
# Heuristic function: detect dealer customer numbers which often are numeric possibly with separators.def is_dealer_customer_number(s):    # Remove separators like '-' and '/'.    s_clean = s.replace('-','').replace('/','')    # Treat as dealer customer number if it's numeric and within expected length.    return s_clean.isdigit() and 5 <= len(s_clean) <= 12
# Set of known makes to get high precision on make detection; extend with your domain's makes.KNOWN_MAKES = set(["sony","samsung","lg","dell","hp","lenovo","bosch","bajaj","maruti","tata"]) 
# Heuristic function: determine if token is a 'make' (brand).def is_make(s):    # Keep only alphabetic characters and convert to lowercase for normalization.    s_clean = re.sub(r"[^A-Za-z]","", s).lower()    # If token matches our known list, it's definitely a make.    if s_clean in KNOWN_MAKES:        return True    # Otherwise, if it's alphabetic and reasonable length assume it might be a make.    return 2 <= len(s_clean) <= 20 and s_clean.isalpha()
# Main labeling function applying heuristics in a priority order.# Priority matters: more specific patterns first (serials) then looser (make).def label_text(s):    # Ensure we work with a trimmed string to avoid whitespace artifacts.    s = str(s).strip()    # Prefer serial detection first (specific and high impact).    if is_serial(s):        return 'serial_number'    # Next, check for customer IDs which are often numeric or patterned.    if is_customer_id(s):        return 'customer_id'    # Next, handle dealer customer numbers.    if is_dealer_customer_number(s):        return 'dealer_customer_number'    # Dealer codes are short alphanumeric; check them next.    if is_dealer_code(s):        return 'dealer_code'    # Finally, check if it is a make.    if is_make(s):        return 'make'    # Fallback label when heuristics can't decide.    return 'unknown'
# Utility to split compound input strings into candidate tokens and label each.# Many raw rows contain multiple fields separated by commas/pipes; this attempts to split them.def split_and_label(text):    # Split on common delimiters (pipe, comma, semicolon, tab).    parts = re.split(r"[|,;\t]+", text)    # Trim parts and remove empty tokens.    parts = [p.strip() for p in parts if p.strip()]    # If splitting doesn't yield multiple parts, fall back to whitespace splitting.    if len(parts) <= 1:        parts = text.split()    labeled = []    # Label each candidate token using heuristics above.    for p in parts:        labeled.append((p, label_text(p)))    return labeled
# The CLI entry point for data_prep.py. Reads raw CSV and outputs token-level labeled CSV.def main(input_csv, out_csv, text_col='text', max_rows=None):    # Load the CSV; nrows optional for debugging with a subset.    df = pd.read_csv(input_csv, nrows=max_rows)    # If the expected column doesn't exist, assume single-column CSV and rename it to 'text'.    if text_col not in df.columns:        df.columns = ['text']    rows = []    # Iterate rows with a progress bar to provide feedback on large files.    for _, r in tqdm(df.iterrows(), total=len(df)):        # Extract text from the chosen column for this row.        text = r[text_col]        # Split and label into (token,label) pairs.        labeled = split_and_label(text)        # Append each token-label as a separate row for downstream supervised training.        for token, label in labeled:            rows.append({'text': token, 'label': label})    # Convert collected rows into a dataframe and write to CSV for inspection and training.    out = pd.DataFrame(rows)    out.to_csv(out_csv, index=False)    # Informative print to tell the user how many token-level rows were produced.    print(f"Saved {len(out)} labeled rows to {out_csv}")
# Only execute main when running as a script, not when imported as a module.if __name__ == '__main__':    # Set up command-line arguments to let user specify input and output paths.    p = argparse.ArgumentParser()    p.add_argument('--input', required=True)    p.add_argument('--out', required=True)    p.add_argument('--text_col', default='text')    p.add_argument('--max_rows', type=int, default=None)    args = p.parse_args()    # Call main with parsed arguments.    main(args.input, args.out, args.text_col, args.max_rows)
# --------------------# file: dataset.py# --------------------# Import PyTorch core to handle tensors and dataset constructs.import torch# Import Dataset base class to create custom dataset object.from torch.utils.data import Dataset# Import tokenizer helper from Hugging Face to transform text into model inputs.from transformers import AutoTokenizer
# Define a dataset object that yields tokenized inputs and labels for training.class FieldDataset(Dataset):    # Initialize with CSV path, optional label mapping, and tokenizer/model parameters.    def __init__(self, csv_file, label2id=None, model_name='distilbert-base-uncased', max_len=64):        # Import pandas locally so module import remains lightweight if not used.        import pandas as pd        # Read CSV into a dataframe for indexing.        self.df = pd.read_csv(csv_file)        # Ensure required columns are present; otherwise raise a clear error.        if 'label' not in self.df.columns or 'text' not in self.df.columns:            raise ValueError('CSV must have columns: text,label')        # If no label2id provided, infer mapping by sorting unique labels for deterministic ordering.        if label2id is None:            labels = sorted(self.df['label'].unique())            self.label2id = {l:i for i,l in enumerate(labels)}        else:            # Use provided mapping to ensure consistency across train/eval/predict runs.            self.label2id = label2id        # Create reverse mapping for convenience.        self.id2label = {v:k for k,v in self.label2id.items()}        # Instantiate tokenizer corresponding to the model backbone.        self.tokenizer = AutoTokenizer.from_pretrained(model_name)        # Store maximum token sequence length for padding/truncation.        self.max_len = max_len
    # Return dataset size to allow DataLoader to know how many samples exist.    def __len__(self):        return len(self.df)
    # Implement item access to return a dictionary of tensors required by the model.    def __getitem__(self, idx):        # Fetch the row by integer position.        row = self.df.iloc[idx]        # Ensure text is string.        text = str(row['text'])        # Map label to integer ID, defaulting to 'unknown' id if label is unseen.        label = row['label']        label_id = self.label2id.get(label, self.label2id.get('unknown', len(self.label2id)))        # Tokenize with padding/truncation to fixed max_length and return PyTorch tensors.        enc = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')        # Squeeze batch dim because tokenizer returns batched tensors by default.        item = {k: v.squeeze(0) for k,v in enc.items()}        # Attach label tensor for training loss computation.        item['labels'] = torch.tensor(label_id, dtype=torch.long)        return item
# --------------------# file: model.py# --------------------# Import PyTorch modules for defining neural network layers.import torchimport torch.nn as nn# Import AutoModel to load pre-trained Transformer backbone.from transformers import AutoModel
# Define the classification model wrapping a Transformer encoder and a linear head.class FieldClassifier(nn.Module):    # Constructor: load backbone, create dropout and classification head sized to backbone hidden size.    def __init__(self, model_name='distilbert-base-uncased', num_labels=6, dropout=0.2):        # Call parent constructor to initialize nn.Module internals.        super().__init__()        # Load pre-trained transformer encoder as the backbone for feature extraction.        self.backbone = AutoModel.from_pretrained(model_name)        # Hidden dimension of transformer outputs needed to size the classifier.        hidden_size = self.backbone.config.hidden_size        # Dropout before classifier to reduce overfitting.        self.dropout = nn.Dropout(dropout)        # Linear layer maps pooled representation to label logits.        self.classifier = nn.Linear(hidden_size, num_labels)
    # Forward pass that optionally returns loss when labels provided.    def forward(self, input_ids, attention_mask=None, labels=None):        # Pass inputs through transformer to obtain last hidden states.        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)        # Get last hidden state tensor of shape (batch, seq_len, hidden_size).        last_hidden = out.last_hidden_state # (batch, seq, hidden)        # Use the first token's representation (CLS) as pooled sentence embedding.        pooled = last_hidden[:,0,:]        # Apply dropout to pooled representation.        pooled = self.dropout(pooled)        # Compute class logits from pooled representation.        logits = self.classifier(pooled)        # Initialize loss to None; compute if labels supplied (training mode).        loss = None        if labels is not None:            # Use cross-entropy for multi-class classification.            loss_f = nn.CrossEntropyLoss()            loss = loss_f(logits, labels)        # Return dictionary similar to Hugging Face models for consistency.        return {'loss': loss, 'logits': logits}
# --------------------# file: train.py# --------------------# Import standard libraries and training utilities.import argparseimport torchfrom torch.utils.data import DataLoader, random_splitfrom transformers import AdamW, get_linear_schedule_with_warmupfrom dataset import FieldDatasetfrom model import FieldClassifierfrom tqdm import tqdmimport os
# Train function encapsulates dataset creation, model init, training loop, validation, and checkpointing.def train(train_csv, model_out_dir, model_name='distilbert-base-uncased', epochs=3, batch_size=16, lr=2e-5, max_len=64):    # Create dataset instance; it will infer label mapping if not provided.    ds = FieldDataset(train_csv, model_name=model_name, max_len=max_len)    # Save label mapping for later use by predict/eval.    label2id = ds.label2id    id2label = ds.id2label    # Dataset length to compute train/validation split sizes.    n = len(ds)    # Use 10% for validation but at least one sample.    val_size = max( int(0.1*n), 1)    train_size = n - val_size    # Split dataset into train and validation subsets to estimate generalization.    train_ds, val_ds = random_split(ds, [train_size, val_size])
    # Create data loaders for batched iteration.    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)    val_loader = DataLoader(val_ds, batch_size=batch_size)
    # Choose device: GPU if available else CPU.    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    # Number of labels for classification head.    num_labels = len(label2id)    # Initialize model instance and move to device for training.    model = FieldClassifier(model_name=model_name, num_labels=num_labels)    model.to(device)
    # Use AdamW optimizer as recommended for transformer fine-tuning.    optimizer = AdamW(model.parameters(), lr=lr)    # Compute total training steps for learning rate scheduler.    total_steps = epochs * len(train_loader)    # Linear schedule with warmup is common in transformer fine-tuning.    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)
    # Track best validation accuracy to save best checkpoint.    best_val_acc = 0.0    # Ensure output directory exists for saving models.    os.makedirs(model_out_dir, exist_ok=True)    # Loop epochs for iterative improvement.    for epoch in range(epochs):        # Set model to training mode so dropout and other layers behave appropriately.        model.train()        # Progress bar over batches improves monitoring.        pbar = tqdm(train_loader, desc=f"Train E{epoch+1}")        total_loss = 0.0        # Iterate batches.        for batch in pbar:            # Move inputs and labels to the selected device.            input_ids = batch['input_ids'].to(device)            attn = batch['attention_mask'].to(device)            labels = batch['labels'].to(device)            # Forward pass returns loss and logits.            out = model(input_ids=input_ids, attention_mask=attn, labels=labels)            loss = out['loss']            # Zero gradients to prevent accumulation.            optimizer.zero_grad()            # Backpropagate loss to compute gradients.            loss.backward()            # Optimizer step updates model parameters.            optimizer.step()            # Scheduler step adjusts learning rate per step.            scheduler.step()            # Accumulate loss for logging.            total_loss += loss.item()            # Update progress bar with running average loss.            pbar.set_postfix({'loss': f"{total_loss / (pbar.n+1):.4f}"})
        # After each epoch evaluate on validation data.        model.eval()        correct = 0        total = 0        # Disable gradient computation during evaluation for speed and memory.        with torch.no_grad():            for batch in val_loader:                # Move batch to device.                input_ids = batch['input_ids'].to(device)                attn = batch['attention_mask'].to(device)                labels = batch['labels'].to(device)                # Get logits from model (no labels argument to save compute of loss).                logits = model(input_ids=input_ids, attention_mask=attn)['logits']                # Predicted class is argmax over logits.                preds = torch.argmax(logits, dim=-1)                # Count correct predictions for accuracy.                correct += (preds == labels).sum().item()                total += labels.size(0)        # Compute validation accuracy; handle empty validation safely.        val_acc = correct/total if total>0 else 0        # Print epoch validation accuracy to monitor overfitting/underfitting.        print(f"Epoch {epoch+1} val_acc={val_acc:.4f}")        # If improved, save checkpoint for best model selection.        if val_acc > best_val_acc:            best_val_acc = val_acc            save_path = os.path.join(model_out_dir, 'best.pt')            # Save model state dict and label mappings to load later for inference.            torch.save({                'model_state_dict': model.state_dict(),                'label2id': label2id,                'id2label': id2label,                'model_name': model_name            }, save_path)            print(f"Saved best model to {save_path}")
    # Save final model regardless, useful for diagnostics or further fine-tuning.    torch.save({        'model_state_dict': model.state_dict(),        'label2id': label2id,        'id2label': id2label,        'model_name': model_name    }, os.path.join(model_out_dir, 'final.pt'))    print('Training finished')
# CLI for train.py when executed as script.if __name__ == '__main__':    # Argument parsing for training parameters and paths.    p = argparse.ArgumentParser()    p.add_argument('--train_csv', required=True)    p.add_argument('--model_out', required=True)    p.add_argument('--epochs', type=int, default=3)    p.add_argument('--batch_size', type=int, default=16)    p.add_argument('--lr', type=float, default=2e-5)    args = p.parse_args()    # Kick off training with provided args.    train(args.train_csv, args.model_out, epochs=args.epochs, batch_size=args.batch_size, lr=args.lr)
# --------------------# file: evaluate.py# --------------------# Import torch for model loading and moving tensors to devices.import torch# argparse for CLI.import argparse# Import FieldDataset to create dataset for evaluation.from dataset import FieldDataset# Import model class to instantiate network architecture.from model import FieldClassifier# DataLoader for batch iteration during evaluation.from torch.utils.data import DataLoader# sklearn metrics for textual classification analysis.from sklearn.metrics import classification_report, confusion_matrix# pandas to display confusion matrix in readable form.import pandas as pd
# Helper to load a saved checkpoint and reconstruct model and label mappings.def load_model(model_path, device='cpu'):    # Load checkpoint map to chosen device to avoid GPU-only pickling issues.    ckpt = torch.load(model_path, map_location=device)    # Read back the model name used during t
