# -*- coding: utf-8 -*-
"""Linear Regression with PyTorch

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12rNZwMgGq5wtLtDYYZN5oRyzFEaJsIS5
"""

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np

# A. Data Generation:
# This section creates a simple, synthetic dataset for linear regression.
# We will model the relationship y = 2*x + 1, with some random noise.
print("1. Generating Synthetic Data...")
# Create a tensor for the input features (x_data)
# We use torch.linspace to generate 100 evenly spaced values between 0 and 10.
X_data = torch.linspace(0, 10, 100).unsqueeze(1) # unsqueeze(1) adds a dimension to make it a (100, 1) tensor
print(f"Shape of X_data: {X_data.shape}")

# Create the target values (y_data) based on the linear equation with added noise.
true_weight = 2.0
true_bias = 1.0
noise = torch.randn(100, 1) * 1.5 # Add random noise
y_data = true_weight * X_data + true_bias + noise
print(f"Shape of y_data: {y_data.shape}")

# B. Model Definition:
# We define a simple linear regression model using PyTorch's nn.Module.
print("\n2. Defining the Model...")
class LinearRegression(nn.Module):
    def __init__(self):
        # The super() call initializes the base class.
        super().__init__()
        # nn.Linear is a module that applies a linear transformation: y = xA^T + b.
        # It automatically creates and manages the weights and biases.
        # The arguments are (in_features, out_features).
        self.linear = nn.Linear(1, 1)

    def forward(self, x):
        # The forward method defines the forward pass of the model.
        # It takes the input tensor 'x' and returns the output.
        return self.linear(x)

# C. Model, Loss Function, and Optimizer Initialization:
# Here we instantiate the model and define the tools needed for training.
print("\n3. Initializing Model, Loss, and Optimizer...")
# Instantiate the model
model = LinearRegression()

# Define the Loss Function (Criterion):
# We use Mean Squared Error (MSE) which is standard for regression tasks.
# It calculates the average of the squared differences between predicted and actual values.
criterion = nn.MSELoss()

# Define the Optimizer:
# The optimizer is responsible for updating the model's parameters (weights and biases)
# based on the gradients computed during backpropagation.
# We use Stochastic Gradient Descent (SGD) with a specified learning rate.
optimizer = optim.SGD(model.parameters(), lr=0.01)

# D. Training Loop:
# This is the core of the machine learning process. We iterate through the data
# multiple times (epochs) to train the model.
print("\n4. Starting Training Loop...")
epochs = 500
loss_history = []

for epoch in range(epochs):
    # Set the model to training mode. This is important for layers like Dropout or BatchNorm,
    # though it has no effect on a simple linear layer.
    model.train()

    # 1. Forward Pass:
    # Get predictions from the model for the input data.
    predictions = model(X_data)

    # 2. Calculate Loss:
    # Compare the predictions with the actual target values.
    loss = criterion(predictions, y_data)

    # 3. Clear Gradients:
    # Before backpropagation, we must clear the gradients from the previous step.
    # PyTorch accumulates gradients by default.
    optimizer.zero_grad()

    # 4. Backward Pass (Backpropagation):
    # This computes the gradients of the loss with respect to all parameters
    # (weights and biases) in the model.
    loss.backward()

    # 5. Update Parameters:
    # The optimizer uses the calculated gradients to update the model's parameters.
    optimizer.step()

    # Log and store the loss to track training progress.
    loss_history.append(loss.item())
    if (epoch + 1) % 50 == 0:
        print(f"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}")

# E. Evaluation and Visualization:
# After training, we evaluate the model's performance and visualize the results.
print("\n5. Evaluating and Visualizing Results...")
# Set the model to evaluation mode. This disables features like dropout, which are
# only used during training. It's a best practice.
model.eval()

# To get predictions without computing gradients, we use torch.no_grad().
# This makes the computation more efficient and prevents accidental updates.
with torch.no_grad():
    # Make final predictions using the trained model
    predicted_y = model(X_data).detach().numpy()

# Get the learned parameters (weight and bias) from the trained model
learned_weight = model.linear.weight.item()
learned_bias = model.linear.bias.item()

print(f"Original Parameters: Weight={true_weight}, Bias={true_bias}")
print(f"Learned Parameters: Weight={learned_weight:.4f}, Bias={learned_bias:.4f}")

# Plotting the results
plt.figure(figsize=(10, 6))
# Plot the original data points
plt.scatter(X_data.numpy(), y_data.numpy(), label='Original Data')
# Plot the fitted line
plt.plot(X_data.numpy(), predicted_y, color='red', linewidth=3, label='Fitted Line')
plt.title('Linear Regression with PyTorch')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.grid(True)
plt.show()

# Plot the training loss history
plt.figure(figsize=(10, 6))
plt.plot(loss_history)
plt.title('Training Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.grid(True)
plt.show()